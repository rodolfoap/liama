# Liama.cpp

A docker container to run a CPU uncensored Vicuna model using https://github.com/ggerganov/llama.cpp. Evidently, it can be used with alternative models.

### Building

```
./build.bash
```

### Launching

```
./launch.bash

...

>
```
